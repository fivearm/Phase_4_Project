{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "check.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fivearm/Phase_4_Project/blob/ninas_branch/check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa6d90c",
        "outputId": "700b720f-4798-4f0f-a68f-79e88b791db3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "#from statsmodels.tsa.arima.model import ARIMA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "%matplotlib inline"
      ],
      "id": "daa6d90c",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d4322af"
      },
      "source": [
        "home_value_old = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/zillow_data.csv')\n",
        "home_value_new = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
        "\n",
        "mean_listing_price_cut = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/Metro_mean_listings_price_cut_amt_uc_sfrcondo_sm_month.csv')\n",
        "median_listing_price_cut = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/Metro_med_listings_price_cut_amt_uc_sfrcondo_sm_month.csv')\n",
        "perc_listing_with_price_cut = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/Metro_perc_listings_price_cut_uc_sfrcondo_sm_month.csv')\n",
        "\n",
        "for_sale_inv_smoothed = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/Metro_invt_fs_uc_sfrcondo_sm_month.csv')\n",
        "\n",
        "median_sale_price = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/Metro_median_sale_price_uc_sfrcondo_sm_sa_month.csv')\n",
        "median_listing_price= pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/median_list_price.csv')\n",
        "\n",
        "newly_pending_listings = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/newly_pending_listings.csv')\n",
        "mean_days_to_pending = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/mean_days_to_pending.csv')\n",
        "median_days_to_pending = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/median_days_to_pending.csv')\n",
        "\n",
        "lumber = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/plywood_prices.csv').rename({'WPU083': 'plywood prices', 'DATE': 'time'}, axis=1).set_index('time')\n",
        "concrete = pd.read_csv('/content/gdrive/MyDrive/Phase_4_Project/data/concrete_prices.csv').rename({'PCU32733273': 'concrete prices', 'DATE' : 'time'}, axis = 1).set_index('time')\n",
        "\n",
        "\n",
        "new_dfs = [for_sale_inv_smoothed\n",
        "          ,mean_listing_price_cut\n",
        "          ,median_listing_price_cut\n",
        "          ,median_sale_price\n",
        "          ,perc_listing_with_price_cut\n",
        "          ,newly_pending_listings\n",
        "          ,mean_days_to_pending\n",
        "          ,median_days_to_pending\n",
        "          ,median_listing_price\n",
        "          ,home_value_old\n",
        "          ,home_value_new]\n",
        "          \n",
        "\n",
        "non_date_cols = [col \n",
        "                  for df in new_dfs \n",
        "                 for col in df.columns\n",
        "                 if not col[:3].isdigit()]\n",
        "id_cols = list(set(non_date_cols))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "3d4322af",
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64d43e30"
      },
      "source": [
        "def melt_data(df, compress = False, _id_vars=None):\n",
        "    \"\"\"\n",
        "    Takes the zillow_data dataset in wide form or a subset of the zillow_dataset.  \n",
        "    Returns a long-form datetime dataframe \n",
        "    with the datetime column names as the index and the values as the 'values' column.\n",
        "    \n",
        "    If more than one row is passes in the wide-form dataset, the values column\n",
        "    will be the mean of the values from the datetime columns in all of the rows.\n",
        "    \n",
        "    If compress is True will compress data to only the value column.\n",
        "    \"\"\"\n",
        "    if not _id_vars:\n",
        "      _id_vars = ['RegionName', 'RegionID', 'SizeRank', 'City', 'State', \n",
        "                                        'Metro', 'CountyName']\n",
        "    present_vars = df.columns[df.columns.isin(_id_vars)]\n",
        "    if not present_vars.empty:\n",
        "\n",
        "      melted = pd.melt(df,id_vars=present_vars, var_name='time')\n",
        "    else:\n",
        "      melted = pd.melt(df, var_name='time')\n",
        "    \n",
        "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True).apply(lambda x: x.strftime('%Y-%m'))\n",
        "    melted = melted.dropna(subset=['value'])    \n",
        "    resampled = melted.groupby(['RegionID','time'])['value'].mean()\n",
        "\n",
        "    return resampled"
      ],
      "id": "64d43e30",
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58QAySkERV-T"
      },
      "source": [
        "melted_new = [melt_data(df, _id_vars=id_cols) for df in new_dfs]\n",
        "\n",
        "names = ['for_sale_inv'\n",
        "          ,'mean_listing_price_cut'\n",
        "          ,'median_listing_price_cut'\n",
        "          ,'median_sale_price'\n",
        "          ,'perc_listing_with_price_cut'\n",
        "          ,'newly_pending_listings'\n",
        "          ,'mean_days_to_pending'\n",
        "          ,'median_days_to_pending'\n",
        "          ,'median_listing_price'\n",
        "          ,'home_value'\n",
        "          ,'home_value'\n",
        "          ]\n",
        "combined = pd.concat(melted_new,axis='columns', join='outer', keys=names).dropna(how='all')\n",
        "combined"
      ],
      "id": "58QAySkERV-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7DTeaRjSdv6"
      },
      "source": [
        "combined.iloc[:,-2] = combined.iloc[:,-2].fillna(combined.iloc[:,-1])\n",
        "combined_filtered = combined.iloc[:,:-1]"
      ],
      "id": "x7DTeaRjSdv6",
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si0ZNdd81QsE",
        "outputId": "10c7e6d0-40cb-4e69-aede-8db70c75fa2b"
      },
      "source": [
        "con_lum = pd.concat([concrete, lumber], axis=1)\n",
        "con_lum.index = pd.to_datetime(con_lum.index)\n",
        "con_lum_resamp = con_lum.resample('M').mean().reset_index()\n",
        "con_lum_resamp['time'] = con_lum_resamp['time'].apply(lambda x: x.strftime('%Y-%m'))\n",
        "\n",
        "final_df = combined_filtered.reset_index().merge(con_lum_resamp, right_on='time', left_on='time', how='left').set_index(['RegionID', 'time']).dropna(thresh=8)\n",
        "\n",
        "final_df.corr()['home_value']"
      ],
      "id": "si0ZNdd81QsE",
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "for_sale_inv                  -0.008658\n",
              "mean_listing_price_cut         0.851738\n",
              "median_listing_price_cut       0.869375\n",
              "median_sale_price              0.985147\n",
              "perc_listing_with_price_cut   -0.070317\n",
              "newly_pending_listings         0.143695\n",
              "mean_days_to_pending          -0.259710\n",
              "median_days_to_pending        -0.154947\n",
              "median_listing_price           0.945003\n",
              "home_value                     1.000000\n",
              "concrete prices                0.022859\n",
              "plywood prices                 0.022047\n",
              "Name: home_value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7977010"
      },
      "source": [
        "def kpss_test(df, name = None):\n",
        "    '''\n",
        "    Will return the kpss test\n",
        "    \n",
        "    Parmeters\n",
        "    ----------\n",
        "    df = dataframe or series\n",
        "    name = Name of the variable you are testing\n",
        "    '''\n",
        "    df_test = kpss(np.log(df), nlags='auto')\n",
        "    print(f'Is {name} data stationary?')\n",
        "    print(f'Test stastic = {df_test[0]: .3f}')\n",
        "    print(f'P-value = {df_test[1]: .3f}')\n",
        "    print('Critical Values: ')\n",
        "    for k, v in df_test[3].items():\n",
        "        print(f'{k}: {v}')"
      ],
      "id": "c7977010",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79b6b687"
      },
      "source": [
        "## Stationarize and Outlier Handling\n"
      ],
      "id": "79b6b687"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhPMSqtxfoiW"
      },
      "source": [
        "def is_stationary(df):\n",
        "  df_test = adfuller(df)\n",
        "  critical_95 = df_test[4]['5%']\n",
        "  return critical_95 > df_test[0] \n",
        "\n",
        "def make_stationary(df):\n",
        "  log_transf = np.log2(df)\n",
        "  \n",
        "  diff = log_transf.diff().dropna()\n",
        "  while is_stationary(diff) == False:\n",
        "    diff = diff.diff().dropna()\n",
        "  diff.name = df.name+ '_stationary'\n",
        "  return diff"
      ],
      "id": "jhPMSqtxfoiW",
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQjf9oCJDc3"
      },
      "source": [
        "### Looks like plywood and cement prices are non stationary"
      ],
      "id": "lkQjf9oCJDc3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTI4Xlh1GBiW",
        "outputId": "806eac29-5351-4c8a-f4bb-e7e772343204"
      },
      "source": [
        "for col in final_df.columns:\n",
        "  if is_stationary(final_df[col].dropna()) == False:\n",
        "    print(f'{col} is non stationary')"
      ],
      "id": "QTI4Xlh1GBiW",
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "concrete prices is non stationary\n",
            "plywood prices is non stationary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8EyCO8qJ2rM"
      },
      "source": [
        " stationary_concrete = make_stationary(final_df['concrete prices'])\n",
        " stationary_plywood = make_stationary(final_df['plywood prices'])\n",
        " \n",
        "final_df = final_df.merge(stationary_concrete,left_index=True, right_index=True)\n",
        "final_df = final_df.merge(stationary_plywood,left_index=True, right_index=True).drop(columns=['concrete prices','plywood prices'])"
      ],
      "id": "m8EyCO8qJ2rM",
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "3b391339",
        "outputId": "de4ae092-4912-4170-8e28-a78db28073ef"
      },
      "source": [
        "def replace_with_median(col, lower_threshold, median):\n",
        "    col[col > -lower_threshold] =  median\n",
        "    col[col < lower_threshold] = median\n",
        "    return col\n",
        "\n",
        "def get_replacement_vals(col):\n",
        "      std = col.std(ddof=0)\n",
        "      mean = col.mean()\n",
        "      median = col.median()\n",
        "      lower_thresh = (-3 * std)+ mean\n",
        "      return median, lower_thresh\n",
        "\n",
        "def replace_outliers(train, test=pd.DataFrame()):\n",
        "  train_copy = train.copy()\n",
        "  median, lower_thresh = get_replacement_vals(train_copy)\n",
        "  stand_train = replace_with_median(train_copy, lower_thresh, median)\n",
        "\n",
        "  if not test.empty:\n",
        "    test_copy = test.copy()\n",
        "    stand_test = replace_with_median(test_copy, lower_thresh, median)\n",
        "    return stand_train, stand_test\n",
        "  \n",
        "  return stand_train\n"
      ],
      "id": "3b391339",
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'value'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-210-b32ad5d0dbe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mstand_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mby_month\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mstationary_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_stationary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby_month\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'value'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "350adfa2"
      },
      "source": [
        "## Cross Validation"
      ],
      "id": "350adfa2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "375fb02b"
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    sum_squared_diff = sum([(y2 - y1)**2 for y2,y1 in zip(y_true, y_pred)])\n",
        "    rmse = (sum_squared_diff * (1/len(y_true))) ** 0.5\n",
        "    return rmse\n",
        "\n",
        "def cross_validate(model, df, split_freq='M', model_kwargs={}):\n",
        "    '''\n",
        "    Resamples dataframe using aggregate mean\n",
        "    Preforms cross validation of the input model. \n",
        "    ---------------------------------------------------------------------\n",
        "    model: uninitalized model- no parantheses \n",
        "    split_freq: the frequency to index the data by\n",
        "    model_kwargs: keyword arguments to pass into the model before fitting\n",
        "    ---------------------------------------------------------------------\n",
        "    Returns: An array of RMSE\n",
        "    '''\n",
        "    \n",
        "    df = df.resample(split_freq).mean()\n",
        "    all_rmse = []\n",
        "    # Create and loop through 4 different train and validation sets\n",
        "    split = TimeSeriesSplit(n_splits=4)\n",
        "    for train_ind, val_ind in split.split(df):\n",
        "        # Initialize, fit, and generate predictions the model on subset of df\n",
        "        model_fit = model(df.iloc[train_ind], **model_kwargs).fit()\n",
        "        preds = model_fit.predict(typ='levels', start=val_ind[0], end=val_ind[-1])\n",
        "        \n",
        "        # Grab Y actual from df\n",
        "        true = df.iloc[val_ind]\n",
        "        \n",
        "        # Calculate and store RMSE\n",
        "        all_rmse.append(rmse(true, preds))\n",
        "    return all_rmse \n",
        "\n",
        "cross_validate(ARIMA, Miami_df['value'], 'M', model_kwargs={'order':(1,2,0)})"
      ],
      "id": "375fb02b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beb16c74"
      },
      "source": [
        "## Arima "
      ],
      "id": "beb16c74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87a86f47"
      },
      "source": [
        "def arima(series, order):\n",
        "    '''\n",
        "    order = (p,d,q)\n",
        "    p: The number of lag observations included in the model, also called the lag order.\n",
        "    d: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
        "    q: The size of the moving average window, also called the order of moving average\n",
        "    '''\n",
        "    model = ARIMA(series.values, order=order).fit()\n",
        "    print(model.summary())\n",
        "    \n",
        "    residuals = model.resid\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(series.index, residuals)"
      ],
      "id": "87a86f47",
      "execution_count": null,
      "outputs": []
    }
  ]
}